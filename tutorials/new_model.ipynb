{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3724ae-9836-45f7-ac5c-8f4704cca50e",
   "metadata": {},
   "source": [
    "# Train a new model on your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde45c56-33df-4d65-9d72-654e59aea905",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_beam_search import seq2seq\n",
    "from post_ocr_correction import correction\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1237c76-7fa6-4670-8e47-c5124cb07e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "\n",
    "source = [list(\"abcdefghijkl\"), list(\"mnopqrstwxyz\")]\n",
    "target = [list(\"abcdefghijk\"), list(\"mnopqrstwxy\")]\n",
    "\n",
    "source_index = seq2seq.Index(source)\n",
    "target_index = seq2seq.Index(target)\n",
    "\n",
    "X = source_index.text2tensor(source)\n",
    "Y = target_index.text2tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa5bb25-5c85-45a8-a2cb-1a26ac9a11fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jarobyte/anaconda3/envs/poc/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2Seq Transformer\n",
      "Source index: <Seq2Seq Index with 28 items>\n",
      "Target index: <Seq2Seq Index with 26 items>\n",
      "Max sequence length: 32\n",
      "Embedding dimension: 32\n",
      "Feedforward dimension: 128\n",
      "Encoder layers: 2\n",
      "Decoder layers: 2\n",
      "Attention heads: 2\n",
      "Activation: relu\n",
      "Dropout: 0.0\n",
      "Trainable parameters: 63,130\n",
      "\n",
      "Training started\n",
      "X_train.shape: torch.Size([2, 14])\n",
      "Y_train.shape: torch.Size([2, 13])\n",
      "Epochs: 100\n",
      "Learning rate: 0.0001\n",
      "Weight decay: 0\n",
      "Epoch | Train                 | Minutes\n",
      "      | Loss     | Error Rate |\n",
      "---------------------------------------\n",
      "    1 |   3.5962 |    100.000 |     0.0\n",
      "    2 |   3.5683 |    100.000 |     0.0\n",
      "    3 |   3.5404 |    100.000 |     0.0\n",
      "    4 |   3.5127 |    100.000 |     0.0\n",
      "    5 |   3.4848 |     95.833 |     0.0\n",
      "    6 |   3.4571 |     95.833 |     0.0\n",
      "    7 |   3.4295 |     95.833 |     0.0\n",
      "    8 |   3.4020 |     95.833 |     0.0\n",
      "    9 |   3.3747 |     95.833 |     0.0\n",
      "   10 |   3.3479 |     95.833 |     0.0\n",
      "   11 |   3.3215 |     95.833 |     0.0\n",
      "   12 |   3.2961 |     95.833 |     0.0\n",
      "   13 |   3.2711 |     95.833 |     0.0\n",
      "   14 |   3.2467 |     91.667 |     0.0\n",
      "   15 |   3.2226 |     91.667 |     0.0\n",
      "   16 |   3.1987 |     87.500 |     0.0\n",
      "   17 |   3.1752 |     87.500 |     0.0\n",
      "   18 |   3.1521 |     83.333 |     0.0\n",
      "   19 |   3.1294 |     83.333 |     0.0\n",
      "   20 |   3.1069 |     83.333 |     0.0\n",
      "   21 |   3.0850 |     83.333 |     0.0\n",
      "   22 |   3.0634 |     79.167 |     0.0\n",
      "   23 |   3.0421 |     79.167 |     0.0\n",
      "   24 |   3.0214 |     75.000 |     0.0\n",
      "   25 |   3.0012 |     75.000 |     0.0\n",
      "   26 |   2.9813 |     75.000 |     0.0\n",
      "   27 |   2.9617 |     75.000 |     0.0\n",
      "   28 |   2.9423 |     70.833 |     0.0\n",
      "   29 |   2.9232 |     70.833 |     0.0\n",
      "   30 |   2.9042 |     70.833 |     0.0\n",
      "   31 |   2.8853 |     70.833 |     0.0\n",
      "   32 |   2.8666 |     70.833 |     0.0\n",
      "   33 |   2.8480 |     70.833 |     0.0\n",
      "   34 |   2.8293 |     70.833 |     0.0\n",
      "   35 |   2.8106 |     70.833 |     0.0\n",
      "   36 |   2.7919 |     70.833 |     0.0\n",
      "   37 |   2.7732 |     70.833 |     0.0\n",
      "   38 |   2.7546 |     70.833 |     0.0\n",
      "   39 |   2.7359 |     70.833 |     0.0\n",
      "   40 |   2.7174 |     70.833 |     0.0\n",
      "   41 |   2.6990 |     70.833 |     0.0\n",
      "   42 |   2.6805 |     70.833 |     0.0\n",
      "   43 |   2.6621 |     66.667 |     0.0\n",
      "   44 |   2.6436 |     66.667 |     0.0\n",
      "   45 |   2.6251 |     66.667 |     0.0\n",
      "   46 |   2.6068 |     66.667 |     0.0\n",
      "   47 |   2.5888 |     58.333 |     0.0\n",
      "   48 |   2.5708 |     58.333 |     0.0\n",
      "   49 |   2.5531 |     58.333 |     0.0\n",
      "   50 |   2.5355 |     58.333 |     0.0\n",
      "   51 |   2.5181 |     58.333 |     0.0\n",
      "   52 |   2.5008 |     50.000 |     0.0\n",
      "   53 |   2.4834 |     50.000 |     0.0\n",
      "   54 |   2.4662 |     45.833 |     0.0\n",
      "   55 |   2.4490 |     41.667 |     0.0\n",
      "   56 |   2.4321 |     41.667 |     0.0\n",
      "   57 |   2.4152 |     41.667 |     0.0\n",
      "   58 |   2.3984 |     41.667 |     0.0\n",
      "   59 |   2.3816 |     41.667 |     0.0\n",
      "   60 |   2.3649 |     41.667 |     0.0\n",
      "   61 |   2.3483 |     41.667 |     0.0\n",
      "   62 |   2.3316 |     41.667 |     0.0\n",
      "   63 |   2.3150 |     41.667 |     0.0\n",
      "   64 |   2.2985 |     41.667 |     0.0\n",
      "   65 |   2.2821 |     37.500 |     0.0\n",
      "   66 |   2.2659 |     37.500 |     0.0\n",
      "   67 |   2.2497 |     33.333 |     0.0\n",
      "   68 |   2.2336 |     33.333 |     0.0\n",
      "   69 |   2.2176 |     33.333 |     0.0\n",
      "   70 |   2.2018 |     33.333 |     0.0\n",
      "   71 |   2.1861 |     33.333 |     0.0\n",
      "   72 |   2.1704 |     33.333 |     0.0\n",
      "   73 |   2.1549 |     29.167 |     0.0\n",
      "   74 |   2.1395 |     29.167 |     0.0\n",
      "   75 |   2.1242 |     29.167 |     0.0\n",
      "   76 |   2.1090 |     25.000 |     0.0\n",
      "   77 |   2.0939 |     25.000 |     0.0\n",
      "   78 |   2.0789 |     25.000 |     0.0\n",
      "   79 |   2.0641 |     25.000 |     0.0\n",
      "   80 |   2.0494 |     25.000 |     0.0\n",
      "   81 |   2.0349 |     25.000 |     0.0\n",
      "   82 |   2.0205 |     25.000 |     0.0\n",
      "   83 |   2.0062 |     25.000 |     0.0\n",
      "   84 |   1.9919 |     25.000 |     0.0\n",
      "   85 |   1.9779 |     25.000 |     0.0\n",
      "   86 |   1.9639 |     25.000 |     0.0\n",
      "   87 |   1.9501 |     25.000 |     0.0\n",
      "   88 |   1.9364 |     25.000 |     0.0\n",
      "   89 |   1.9228 |     25.000 |     0.0\n",
      "   90 |   1.9092 |     25.000 |     0.0\n",
      "   91 |   1.8958 |     25.000 |     0.0\n",
      "   92 |   1.8825 |     25.000 |     0.0\n",
      "   93 |   1.8693 |     20.833 |     0.0\n",
      "   94 |   1.8561 |     20.833 |     0.0\n",
      "   95 |   1.8431 |     20.833 |     0.0\n",
      "   96 |   1.8303 |     20.833 |     0.0\n",
      "   97 |   1.8175 |     20.833 |     0.0\n",
      "   98 |   1.8048 |     20.833 |     0.0\n",
      "   99 |   1.7922 |     20.833 |     0.0\n",
      "  100 |   1.7798 |     20.833 |     0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (source_embeddings): Embedding(28, 32)\n",
       "  (target_embeddings): Embedding(26, 32)\n",
       "  (positional_embeddings): Embedding(32, 32)\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=32, out_features=128, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (dropout3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=32, out_features=26, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model\n",
    "\n",
    "model = seq2seq.Transformer(source_index, target_index)\n",
    "model.train()\n",
    "model.fit(X, Y, epochs = 100, progress_bar = 0)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "375e3786-ec3c-43c1-8574-1b10b679f5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "\n",
    "test = \"ghijklmnopqrst\"\n",
    "new_source = [list(test)]\n",
    "X_new = source_index.text2tensor(new_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "806c7ab0-59e3-487b-9365-2a20446889ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain beam search\n",
    "\n",
    "predictions, log_probabilities = seq2seq.beam_search(\n",
    "    model, \n",
    "    X_new,\n",
    "    progress_bar = 0\n",
    ")\n",
    "just_beam = target_index.tensor2text(predictions[:, 0, :])[0]\n",
    "just_beam = re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", just_beam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dedf615c-1bc6-4b59-8a5e-e00a721a113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post ocr correction\n",
    "\n",
    "disjoint_beam = correction.disjoint(\n",
    "    test,\n",
    "    model,\n",
    "    source_index,\n",
    "    target_index,\n",
    "    5,\n",
    "    \"beam_search\",\n",
    ")\n",
    "votes, n_grams_beam = correction.n_grams(\n",
    "    test,\n",
    "    model,\n",
    "    source_index,\n",
    "    target_index,\n",
    "    5,\n",
    "    \"beam_search\",\n",
    "    \"triangle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d0bf026-9d86-400c-9cd5-79e86c72a8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "results\n",
      "  test data                       ghijklmnopqrst\n",
      "  plain beam search               mnopqrn\n",
      "  disjoint windows, beam search   mnymrmnopqmnopq\n",
      "  n-grams, beam search, triangle  mnymymoooooymr\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nresults\")\n",
    "print(\"  test data                      \", test)\n",
    "print(\"  plain beam search              \", just_beam)\n",
    "print(\"  disjoint windows, beam search  \", disjoint_beam)\n",
    "print(\"  n-grams, beam search, triangle \", n_grams_beam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
